{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Is it OK to initialize all the weights to the same value as long as that value is selected\n",
        "randomly using He initialization?**\n",
        "\n",
        "**Ans:** Initializing all weights to the same value using He initialization is not recommended. He initialization, which scales the weights based on the number of inputs to a neuron, is designed to prevent the vanishing or exploding gradient problem during training. However, setting all weights to the same value would negate the purpose of He initialization because it would eliminate any diversity in the initial weights.\n",
        "\n",
        "\n",
        "In He initialization, weights are typically initialized randomly from a Gaussian distribution with mean 0 and variance\n",
        "2/\n",
        "number of input neurons\n",
        " (or\n",
        "1/\n",
        "number of input neurons in some variants). This ensures that the initial weights are diverse and have appropriate scales, which helps in training the neural network effectively.\n",
        "\n",
        "\n",
        "Therefore, while using He initialization is a good practice, initializing all weights to the same value would not be advisable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZpADcGu3BfIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Is it OK to initialize the bias terms to 0?**\n",
        "\n",
        "**Ans:** Initializing bias terms to 0 is a common practice in many neural network architectures and is generally considered acceptable. This is because the role of the bias term is to provide the model with the ability to shift the activation function by adding a constant value. Initializing bias terms to 0 at the beginning allows the network to start with a neutral bias and then adjust it during the training process according to the data.\n",
        "\n",
        "Moreover, initializing bias terms to 0 simplifies the initialization process and avoids introducing additional randomness into the model, which can sometimes be beneficial for reproducibility.\n",
        "\n",
        "However, in some cases, especially when dealing with very deep networks or networks with certain activation functions, initializing biases to non-zero values may help the model converge faster or perform better. Experimentation with different initialization strategies, including initializing biases to non-zero values, can sometimes lead to improvements in model performance.\n",
        "\n",
        "In summary, while initializing bias terms to 0 is generally acceptable and widely used, it's worth experimenting with different initialization strategies to see what works best for your specific task and model architecture.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tt44UE19B-DN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Name three advantages of the SELU activation function over ReLU.**\n",
        "\n",
        "**Ans:** The Scaled Exponential Linear Unit (SELU) activation function offers several advantages over the Rectified Linear Unit (ReLU) activation function:\n",
        "\n",
        "**1. Self-normalization:** SELU activation promotes self-normalization in deep neural networks. This means that, under certain conditions (e.g., the weights are initialized according to specific criteria), the activations of each layer tend to converge towards zero mean and unit variance during training. This can mitigate the vanishing or exploding gradient problem, leading to more stable training dynamics and potentially faster convergence.\n",
        "\n",
        "**2. No need for batch normalization:** SELU activation eliminates the need for additional normalization techniques like batch normalization in many cases. This simplifies the network architecture, reduces computational overhead, and makes the model more straightforward to train and deploy.\n",
        "\n",
        "**3. Smoothness and continuity:** SELU is a smooth and continuous function, unlike ReLU, which has a non-differentiable point at zero. This smoothness can be advantageous during optimization, as it allows gradient-based optimization algorithms to navigate the parameter space more effectively, potentially leading to better convergence and performance.\n",
        "\n",
        "Overall, these advantages make SELU a compelling choice, especially for deep neural networks, where stability during training and efficient optimization are critical concerns. However, it's worth noting that SELU may not always outperform ReLU in every scenario, and the choice of activation function should depend on factors such as network architecture, data characteristics, and computational constraints.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GxGdE_3mCJfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. In which cases would you want to use each of the following activation functions: SELU, leaky\n",
        "ReLU (and its variants), ReLU, tanh, logistic, and softmax?**\n",
        "\n",
        "**Ans:**  Activation function:\n",
        "\n",
        "**1. SELU:**\n",
        "\n",
        "* Use SELU when building deep neural networks, especially if you want to leverage its self-normalizing properties to stabilize training and potentially achieve faster convergence. SELU can be particularly useful in scenarios where batch normalization is undesirable or impractical.\n",
        "\n",
        "**2. Leaky ReLU (and its variants):**\n",
        "\n",
        "* Leaky ReLU and its variants (e.g., Parametric ReLU, Randomized Leaky ReLU) are helpful when dealing with the \"dying ReLU\" problem, where neurons can become inactive during training. Use them when you want to introduce a small slope for negative input values to prevent this issue and encourage the flow of gradients during backpropagation. They are particularly useful in scenarios where ReLU tends to result in dead neurons.\n",
        "\n",
        "**3. ReLU:**\n",
        "\n",
        "* ReLU is a widely used activation function and a good default choice for many scenarios. Use ReLU when you want a simple, computationally efficient activation function that tends to perform well in practice. It's especially effective in deep neural networks where vanishing gradients can be a problem with other activation functions like sigmoid or tanh.\n",
        "\n",
        "**4. tanh:**\n",
        "\n",
        "* Use tanh when you need activation values in the range [-1, 1]. It's commonly used in scenarios where you want the output to be normalized between -1 and 1, such as in recurrent neural networks (RNNs) or in the hidden layers of a neural network. It's also useful when you need a smooth activation function that maps input values to a bounded range.\n",
        "\n",
        "**5. Logistic (sigmoid):**\n",
        "\n",
        "* Logistic activation (sigmoid) is primarily used in binary classification tasks where you need to produce probabilities as outputs. It maps input values to the range [0, 1], making it suitable for binary classification problems where you want to predict the probability of an input belonging to a particular class. However, it's less commonly used in hidden layers of deep neural networks due to the vanishing gradient problem.\n",
        "\n",
        "**6. Softmax:**\n",
        "\n",
        "* Softmax is typically used in the output layer of a neural network when you're dealing with multi-class classification problems. It transforms the raw output scores of the network into probabilities, ensuring that the sum of the probabilities across all classes is equal to 1. This makes it suitable for scenarios where you want to output a probability distribution over multiple classes.\n",
        "\n",
        "In summary, the choice of activation function depends on various factors including the nature of the problem, the architecture of the neural network, and computational considerations. Experimentation and empirical validation are often necessary to determine the most suitable activation function for a specific task.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "25sGDLwyCoM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. What may happen if you set theÂ momentumÂ hyperparameter too close to 1 (e.g., 0.99999)\n",
        "when using anÂ SGDÂ optimizer?**\n",
        "\n",
        "**Ans:**\n",
        "Setting the momentum hyperparameter too close to 1, such as 0.99999, when using stochastic gradient descent (SGD) optimizer can lead to several potential issues:\n",
        "\n",
        "**1. Overshooting and Oscillations:** With very high momentum values, the optimizer tends to rely heavily on past gradients, which can cause it to overshoot the minimum point of the loss function. This overshooting behavior can lead to oscillations around the minimum, making it difficult for the optimizer to converge to a stable solution.\n",
        "\n",
        "**2. Slow Convergence:** While momentum helps in accelerating convergence by allowing the optimizer to persist in the direction of past gradients, setting it too close to 1 can result in slow convergence. This is because the optimizer may become overly conservative in updating the weights, especially when encountering steep gradients, which slows down the learning process.\n",
        "\n",
        "**3. Difficulty in Escaping Local Minima:** High momentum values can make it challenging for the optimizer to escape local minima or saddle points in the loss landscape. Instead, the optimizer may get trapped in these suboptimal points due to its strong reliance on past gradients, hindering the exploration of the parameter space.\n",
        "\n",
        "**4. Sensitivity to Noise:** Extremely high momentum values can make the optimizer more sensitive to noise in the gradients, leading to erratic behavior during training. Small fluctuations in the gradients can have a significant impact on the weight updates, causing instability in the optimization process.\n",
        "\n",
        "In summary, while momentum can be a powerful tool for accelerating convergence and improving the performance of SGD, setting it too close to 1 can introduce instability and hinder the optimization process. It's essential to carefully tune the momentum hyperparameter based on the characteristics of the dataset and the specific optimization goals to achieve optimal performance.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vwrKJjqGC1Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Name three ways you can produce a sparse model.**\n",
        "\n",
        "\n",
        "Creating a sparse model, where a significant portion of weights are zero, can be beneficial for reducing memory footprint, accelerating inference, and improving model interpretability. Here are three ways to produce a sparse model:\n",
        "\n",
        "Pruning:\n",
        "Pruning involves identifying and removing the less important weights from a trained model. This can be done based on various criteria such as magnitude, connectivity, or sensitivity to perturbations. By pruning away weights with small magnitudes or low importance, a sparse model with a significant number of zero weights can be obtained. Pruning can be performed iteratively during training or as a post-training optimization step.\n",
        "Regularization:\n",
        "Regularization techniques such as\n",
        "ð¿\n",
        "1\n",
        "L\n",
        "1\n",
        "â€‹\n",
        "  regularization (Lasso regularization) encourage sparsity by adding a penalty term to the loss function that penalizes large weight magnitudes. By minimizing this penalty term, the model is incentivized to learn sparse representations, leading to a higher proportion of zero weights.\n",
        "ð¿\n",
        "1\n",
        "L\n",
        "1\n",
        "â€‹\n",
        "  regularization is particularly effective in inducing sparsity because it tends to push many weights to exactly zero.\n",
        "Quantization:\n",
        "Quantization involves reducing the precision of weights and activations in the model, typically from floating-point to lower precision fixed-point representations. In some cases, quantization techniques can lead to a sparse representation where many values become identical, especially when using low-precision integer representations. This results in a model with a higher proportion of zero-valued parameters, thereby achieving sparsity.\n",
        "These methods can be used individually or in combination to produce sparse models with various degrees of sparsity, depending on the specific requirements of the application. Experimentation and fine-tuning may be necessary to achieve the desired level of sparsity while balancing the trade-offs with model performance and accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SW4akOSwEmZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
        "new instances)? What about MC Dropout?**\n",
        "\n",
        "**Ans:** Dropout is a regularization technique commonly used during training to prevent overfitting by randomly dropping (setting to zero) a proportion of neurons' outputs in a layer. Here's how dropout affects training and inference, and the differences with MC Dropout:\n",
        "\n",
        "Training Speed:\n",
        "Dropout can slow down training to some extent because it introduces additional computation. During each training iteration, dropout randomly sets a fraction of neuron activations to zero, effectively reducing the information flow through the network. As a result, the network needs more iterations to converge compared to training without dropout. However, the slowdown is usually manageable and outweighed by the regularization benefits it provides.\n",
        "Inference Speed:\n",
        "Dropout does not affect inference speed since it is only applied during training. During inference (i.e., making predictions on new instances), the entire network is used without dropout. Therefore, inference speed remains unaffected, and dropout does not introduce any computational overhead during prediction time.\n",
        "MC Dropout:\n",
        "MC Dropout (Monte Carlo Dropout) is an extension of dropout that can be used during both training and inference. During inference, MC Dropout involves performing multiple forward passes through the network with dropout enabled and averaging the predictions. This process captures the uncertainty in the model's predictions, providing more reliable estimates of predictive uncertainty. While MC Dropout does introduce additional computation during inference, it can improve the robustness and reliability of the model's predictions, especially in tasks where uncertainty estimation is important, such as in Bayesian neural networks.\n",
        "In summary, dropout can slightly slow down training due to the additional computation involved, but it does not affect inference speed. MC Dropout introduces additional computation during inference but can improve the reliability of predictions by capturing predictive uncertainty. Therefore, the choice of whether to use dropout or MC Dropout depends on the specific requirements of the task, including the need for regularization and uncertainty estimation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xGSxKUScEsrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Practice training a deep neural network on the CIFAR10 image dataset:**\n",
        "\n",
        "**a. Build a DNN with 20 hidden layers of 100 neurons each (thatâ€™s too many, but itâ€™s the\n",
        "point of this exercise). Use He initialization and the ELU activation function.**\n",
        "\n",
        "**b. Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
        "dataset. You can load it withÂ keras.datasets.cifar10.load_â€‹data(). The dataset is\n",
        "composed of 60,000 32 Ã— 32â€“pixel color images (50,000 for training, 10,000 for\n",
        "testing) with 10 classes, so youâ€™ll need a softmax output layer with 10 neurons.\n",
        "Remember to search for the right learning rate each time you change the modelâ€™s\n",
        "architecture or hyperparameters.**\n",
        "\n",
        "**c. Now try adding Batch Normalization and compare the learning curves: Is it\n",
        "converging faster than before? Does it produce a better model? How does it affect\n",
        "training speed?**\n",
        "\n",
        "**d. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
        "to ensure the network self-normalizes (i.e., standardize the input features, use\n",
        "LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
        "layers, etc.).**\n",
        "\n",
        "**e. Try regularizing the model with alpha dropout. Then, without retraining your model,\n",
        "see if you can achieve better accuracy using MC Dropout.**"
      ],
      "metadata": {
        "id": "CxSWtqWCFNXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#a. Build DNN with 20 hidden layers using He initialization and ELU activation\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define the model\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))  # Input layer\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))  # Hidden layers\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))  # Output layer\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "\n",
        "#b. Train the network using Nadam optimization and early stopping:\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"Nadam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_full, y_train_full, epochs=100, validation_split=0.1, callbacks=[early_stopping_cb])\n",
        "\n",
        "\n",
        "\n",
        "#c. Add Batch Normalization:\n",
        "\n",
        "# Update the model to include Batch Normalization\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))  # Input layer\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))  # Hidden layers\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Activation(\"elu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))  # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"Nadam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "history_bn = model.fit(X_train_full, y_train_full, epochs=100, validation_split=0.1, callbacks=[early_stopping_cb])\n",
        "\n",
        "\n",
        "\n",
        "#d. Replace Batch Normalization with SELU:\n",
        "\n",
        "\n",
        "# Update the model to use SELU activation and LeCun normal initialization\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))  # Input layer\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))  # Hidden layers\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))  # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"Nadam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "history_selu = model.fit(X_train_full, y_train_full, epochs=100, validation_split=0.1, callbacks=[early_stopping_cb])\n",
        "\n",
        "\n",
        "\n",
        "#e. Regularize with alpha dropout and explore MC Dropout:\n",
        "\n",
        "# Add alpha dropout\n",
        "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"Nadam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model with alpha dropout\n",
        "history_dropout = model.fit(X_train_full, y_train_full, epochs=100, validation_split=0.1, callbacks=[early_stopping_cb])\n",
        "\n",
        "# Use MC Dropout\n",
        "class MCDropout(keras.layers.Dropout):\n",
        "    def call(self, inputs):\n",
        "        return super().call(inputs, training=True)\n",
        "\n",
        "# Apply MC Dropout\n",
        "model_mc_dropout = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[32, 32, 3]),  # Input layer\n",
        "    keras.layers.Dropout(rate=0.1),  # Dropout layer\n",
        "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),  # Hidden layer\n",
        "    keras.layers.AlphaDropout(rate=0.1),  # Alpha dropout\n",
        "    keras.layers.Dense(10, activation=\"softmax\")  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_mc_dropout.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"Nadam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Evaluate the model with MC Dropout\n",
        "model_mc_dropout.evaluate(X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nscx2yY_FHwg",
        "outputId": "d9fcdfdb-ced9-4767-baf4-6c9766013c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 3072)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               307300    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 500210 (1.91 MB)\n",
            "Trainable params: 500210 (1.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n",
            "Epoch 1/100\n",
            "1407/1407 [==============================] - 29s 14ms/step - loss: 1.9985 - accuracy: 0.2696 - val_loss: 1.9177 - val_accuracy: 0.2980\n",
            "Epoch 2/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 1.8219 - accuracy: 0.3341 - val_loss: 1.9137 - val_accuracy: 0.3120\n",
            "Epoch 3/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 1.7603 - accuracy: 0.3627 - val_loss: 1.8579 - val_accuracy: 0.3488\n",
            "Epoch 4/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 1.7183 - accuracy: 0.3818 - val_loss: 1.8039 - val_accuracy: 0.3650\n",
            "Epoch 5/100\n",
            "1407/1407 [==============================] - 19s 13ms/step - loss: 1.6861 - accuracy: 0.3960 - val_loss: 1.6910 - val_accuracy: 0.3890\n",
            "Epoch 6/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 1.6619 - accuracy: 0.4064 - val_loss: 1.6678 - val_accuracy: 0.4096\n",
            "Epoch 7/100\n",
            "1407/1407 [==============================] - 19s 13ms/step - loss: 1.6432 - accuracy: 0.4162 - val_loss: 1.7152 - val_accuracy: 0.3980\n",
            "Epoch 8/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 1.6135 - accuracy: 0.4226 - val_loss: 1.7061 - val_accuracy: 0.4046\n",
            "Epoch 9/100\n",
            "1407/1407 [==============================] - 19s 13ms/step - loss: 1.6066 - accuracy: 0.4262 - val_loss: 1.6698 - val_accuracy: 0.4206\n",
            "Epoch 10/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5763 - accuracy: 0.4388 - val_loss: 1.6552 - val_accuracy: 0.4206\n",
            "Epoch 11/100\n",
            "1407/1407 [==============================] - 19s 13ms/step - loss: 4.1264 - accuracy: 0.3339 - val_loss: 1.9938 - val_accuracy: 0.2310\n",
            "Epoch 12/100\n",
            "1407/1407 [==============================] - 21s 15ms/step - loss: 1.9058 - accuracy: 0.2667 - val_loss: 1.8506 - val_accuracy: 0.3184\n",
            "Epoch 13/100\n",
            "1407/1407 [==============================] - 19s 13ms/step - loss: 1.7998 - accuracy: 0.3284 - val_loss: 1.7575 - val_accuracy: 0.3490\n",
            "Epoch 14/100\n",
            "1407/1407 [==============================] - 19s 13ms/step - loss: 1.7385 - accuracy: 0.3600 - val_loss: 1.7568 - val_accuracy: 0.3608\n",
            "Epoch 15/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 1.7063 - accuracy: 0.3746 - val_loss: 1.7862 - val_accuracy: 0.3578\n",
            "Epoch 16/100\n",
            "1407/1407 [==============================] - 19s 14ms/step - loss: 1.6729 - accuracy: 0.3880 - val_loss: 1.6909 - val_accuracy: 0.3832\n",
            "Epoch 17/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 1.6519 - accuracy: 0.4021 - val_loss: 1.6999 - val_accuracy: 0.3876\n",
            "Epoch 18/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 1.6320 - accuracy: 0.4095 - val_loss: 1.6816 - val_accuracy: 0.3928\n",
            "Epoch 19/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 1.6187 - accuracy: 0.4148 - val_loss: 1.6660 - val_accuracy: 0.4034\n",
            "Epoch 20/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6005 - accuracy: 0.4241 - val_loss: 1.6456 - val_accuracy: 0.4122\n",
            "Epoch 21/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5862 - accuracy: 0.4302 - val_loss: 1.6508 - val_accuracy: 0.4170\n",
            "Epoch 22/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 6.0288 - accuracy: 0.2846 - val_loss: 1.8868 - val_accuracy: 0.2840\n",
            "Epoch 23/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 1.7982 - accuracy: 0.3249 - val_loss: 1.8144 - val_accuracy: 0.3184\n",
            "Epoch 24/100\n",
            "1407/1407 [==============================] - 19s 13ms/step - loss: 1.7561 - accuracy: 0.3497 - val_loss: 1.7489 - val_accuracy: 0.3558\n",
            "Epoch 25/100\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 1.7224 - accuracy: 0.3640 - val_loss: 1.7324 - val_accuracy: 0.3608\n",
            "Epoch 26/100\n",
            "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6893 - accuracy: 0.3765 - val_loss: 1.7135 - val_accuracy: 0.3594\n",
            "Epoch 27/100\n",
            "1407/1407 [==============================] - 24s 17ms/step - loss: 1.6616 - accuracy: 0.3923 - val_loss: 1.8034 - val_accuracy: 0.3574\n",
            "Epoch 28/100\n",
            "1407/1407 [==============================] - 21s 15ms/step - loss: 1.6426 - accuracy: 0.4024 - val_loss: 1.7006 - val_accuracy: 0.3824\n",
            "Epoch 29/100\n",
            "1407/1407 [==============================] - 19s 13ms/step - loss: 6.9253 - accuracy: 0.3887 - val_loss: 2.0614 - val_accuracy: 0.2330\n",
            "Epoch 30/100\n",
            "1407/1407 [==============================] - 21s 15ms/step - loss: 1.9283 - accuracy: 0.2776 - val_loss: 1.8763 - val_accuracy: 0.3148\n",
            "Epoch 1/100\n",
            "1407/1407 [==============================] - 52s 22ms/step - loss: 1.8400 - accuracy: 0.3389 - val_loss: 1.7789 - val_accuracy: 0.3688\n",
            "Epoch 2/100\n",
            "1407/1407 [==============================] - 29s 21ms/step - loss: 1.6930 - accuracy: 0.3970 - val_loss: 1.7017 - val_accuracy: 0.3986\n",
            "Epoch 3/100\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.6236 - accuracy: 0.4248 - val_loss: 1.6316 - val_accuracy: 0.4246\n",
            "Epoch 4/100\n",
            "1407/1407 [==============================] - 28s 20ms/step - loss: 1.5729 - accuracy: 0.4438 - val_loss: 1.6753 - val_accuracy: 0.4116\n",
            "Epoch 5/100\n",
            "1407/1407 [==============================] - 29s 21ms/step - loss: 1.5260 - accuracy: 0.4608 - val_loss: 1.7077 - val_accuracy: 0.4230\n",
            "Epoch 6/100\n",
            "1407/1407 [==============================] - 30s 21ms/step - loss: 1.4886 - accuracy: 0.4726 - val_loss: 1.7332 - val_accuracy: 0.3882\n",
            "Epoch 7/100\n",
            "1407/1407 [==============================] - 29s 20ms/step - loss: 1.4517 - accuracy: 0.4876 - val_loss: 1.6202 - val_accuracy: 0.4306\n",
            "Epoch 8/100\n",
            "1407/1407 [==============================] - 29s 20ms/step - loss: 1.4212 - accuracy: 0.4995 - val_loss: 1.5148 - val_accuracy: 0.4710\n",
            "Epoch 9/100\n",
            "1407/1407 [==============================] - 29s 21ms/step - loss: 1.3932 - accuracy: 0.5062 - val_loss: 1.5634 - val_accuracy: 0.4608\n",
            "Epoch 10/100\n",
            "1407/1407 [==============================] - 30s 22ms/step - loss: 1.3684 - accuracy: 0.5176 - val_loss: 1.5569 - val_accuracy: 0.4474\n",
            "Epoch 11/100\n",
            "1407/1407 [==============================] - 29s 21ms/step - loss: 1.3431 - accuracy: 0.5254 - val_loss: 1.4731 - val_accuracy: 0.4770\n",
            "Epoch 12/100\n",
            "1407/1407 [==============================] - 30s 21ms/step - loss: 1.3201 - accuracy: 0.5334 - val_loss: 1.4212 - val_accuracy: 0.4976\n",
            "Epoch 13/100\n",
            "1407/1407 [==============================] - 29s 21ms/step - loss: 1.2986 - accuracy: 0.5416 - val_loss: 1.4905 - val_accuracy: 0.4976\n",
            "Epoch 14/100\n",
            "1407/1407 [==============================] - 28s 20ms/step - loss: 1.2802 - accuracy: 0.5500 - val_loss: 1.4774 - val_accuracy: 0.4976\n",
            "Epoch 15/100\n",
            "1407/1407 [==============================] - 29s 21ms/step - loss: 1.2606 - accuracy: 0.5543 - val_loss: 1.5327 - val_accuracy: 0.4716\n",
            "Epoch 16/100\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2480 - accuracy: 0.5601 - val_loss: 1.5454 - val_accuracy: 0.4774\n",
            "Epoch 17/100\n",
            "1407/1407 [==============================] - 29s 21ms/step - loss: 1.2233 - accuracy: 0.5695 - val_loss: 1.8307 - val_accuracy: 0.4214\n",
            "Epoch 18/100\n",
            "1407/1407 [==============================] - 34s 24ms/step - loss: 1.2088 - accuracy: 0.5754 - val_loss: 1.4683 - val_accuracy: 0.4882\n",
            "Epoch 19/100\n",
            "1407/1407 [==============================] - 29s 21ms/step - loss: 1.1944 - accuracy: 0.5806 - val_loss: 1.6206 - val_accuracy: 0.4478\n",
            "Epoch 20/100\n",
            "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1752 - accuracy: 0.5846 - val_loss: 1.5097 - val_accuracy: 0.4874\n",
            "Epoch 21/100\n",
            "1407/1407 [==============================] - 30s 22ms/step - loss: 1.1597 - accuracy: 0.5918 - val_loss: 1.6003 - val_accuracy: 0.4704\n",
            "Epoch 22/100\n",
            "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1491 - accuracy: 0.5963 - val_loss: 1.4041 - val_accuracy: 0.5176\n",
            "Epoch 23/100\n",
            "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1300 - accuracy: 0.6008 - val_loss: 1.5459 - val_accuracy: 0.4836\n",
            "Epoch 24/100\n",
            "1231/1407 [=========================>....] - ETA: 3s - loss: 1.1171 - accuracy: 0.6074"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1JQpYKuPGqaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}